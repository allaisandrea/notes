\documentclass[11pt]{article}
\newcommand{\dd}{\mathrm{d}}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx, caption}

\begin{document}
\section{Confidence intervals} 

\subsection{Definition}

Let's say we have collected some observations $x$, for example a few die rolls:
$x = (1, 3, 3, 6)$. Supposedly, these observation are distributed according to
some parametric model with unknown parameters $\theta$. In the case of the die
roll, the model could be the categorical distribution, whose parameters are the
probabilities of each category: $\theta = (p_1, \ldots, p_6)$. Based on our
observations, we would like to establish some region of parameter space within
which $\theta$ most likely lies.

More precisely, we want to define a function $J$ that associates to any
possible observation $x$ a region $J(x)$ of parameter space such that ``most
likely'' $\theta \in J(x)$. To give a precise meaning to ``most likely'', we
must first define the notion of \emph{coverage}. The coverage $Q(J, \theta)$ of
$J$ given $\theta$ is the probability, for any given $\theta$, that the
interval $J$ covers $\theta$:

\begin{equation}
    Q(J, \theta) \equiv \sum_{x} p(x|\theta) I(\theta \in J(x))\,.
\end{equation}

Here $p(x|\theta)$ is the probability of each observation $x$ given $\theta$,
and $I(P)$ is the indicator function, which is 1 if the predicate $P$ is true,
and 0 if it is false.

A variation of $J$ that associates smaller regions to each observation yields a
coverage closer to 0, and conversely, a tweak towards larger regions yields a
coverage closer to 1. In the extreme, if $J$ yields the full parameter space
for all observations, the coverage is exactly 1. Ideally, we would like a
function $J_\alpha$ that gives the smallest possible regions, while preserving
at least a  prescribed minimum coverage $\alpha \in(0, 1)$. And crucially, we
would like $J_\alpha$ to do so \emph{regardless} of the true value of $\theta$:

\begin{equation}
    J_\alpha\quad |\quad \forall\, \theta \quad Q(J_\alpha, \theta) \geq \alpha
\end{equation}

This set of requirements is such a tall order that it is worth repeating: the
function $J_\alpha$ is expected to yield the smallest possible regions of
parameters space, while maintaning a minimum coverage, regardless of the
unknown true values of the parameters, just based on the observation $x$.

The region of parameter space that such a function associates to a given
observation $x$ is called \emph{confidence interval} at the confidence level
$\alpha$.

The minimum coverage reqirement for $J$ is so strong, that it often forces
$J_\alpha$ to yield very conservative (large) regions, just to make sure that
the constraint is satisfied in every corner of parameter space. For this reason
sometimes functions $J$ are used that violate the requirement in some
small\footnote{For some ill-defined notion of small.} regions of parameter
space. This allows $J$ to yield less conservative regions, at the price of
having insufficient coverage if the true parameters $\theta$ happen to fall in
the regions of parameters space where the requirement was waived. These kind of
weaker confidence intervals are sometimes characterized as \emph{approximate},
and the stronger confidence intervals as \emph{exact}.

In some cases, depending on the parametric model, it is possible to find a
function $J_\alpha$ that saturates the minimum coverage bound for all values of
$\theta$: $Q(J_\alpha, \theta) = \alpha\quad \forall\, \theta$. This is the
case for observations that follow the normal distribution, but it is not always
possible, and it is never possible for distributions with discrete domain.

Since there is a lot of confusion about confidence intervals, it is also worth
emphasizing what a confidence interval is \emph{not}:

\begin{itemize}

\item The sample mean plus or minus some number times the variance of the
    sample mean (unless the sample mean happens to be normally distributed).

\item A couple of quantiles of the sample mean distribution.

\item A couple of quantiles of the bayesian posterior distribution
    $p(\theta|x)$. Intervals obtained in this way are sometimes called
    \emph{credible intervals}. They are often used in practice as a replacement
    for true confidence intervals, but they have no guaratees of minimum
    coverage.
\end{itemize}

\subsection{Normal distribution}

Let $x = (x_1,\ldots x_n)$ be  $n$ i.i.d. observations from a normal
distribution with mean $\mu$ and variance $\sigma^2$, and let $\bar{x}$, $s^2$
be the sample mean and variance:
\begin{equation}
    \bar{x} = \frac{1}{n} \sum_{i = 1}^{n} x_i \quad \mathrm{and} \quad
    s^2 = \frac{1}{n - 1} \sum_{i = 1}^{n} (x_i - \bar{x})^2\,.
\end{equation}

It can be shown that the quantity:
\begin{equation}
    t =\sqrt{n} \frac{\bar{x} - \mu}{s},
\end{equation}
follows the Student $t$ distribution with $\nu = n - 1$ degrees of freedom:
\begin{equation}
    \mathrm{PDF}_{\mathrm{Student}}(t|\nu) \propto 
    \left(1 + \frac{t^2}{\nu}\right)^{-\frac{\nu + 1}{2}}\,.
\end{equation}

Note that the distribution of the $t$ statistic does \emph{not} depend on the
parameters $(\mu, \sigma)$ of the normal distribution from which the sample $x$
is drawn, it only depends on the sample size $n$. This is a remarkable and
often understated fact. Based on this observation, we define a confidence
interval function
\begin{equation}
    J(x) = \left\{(\mu, \sigma)\ |\
    \bar{x} - \frac{z s}{\sqrt{n}} < \mu
    < \bar{x} + \frac{zs}{\sqrt{n}}\right\}
\end{equation}
where $z > 0$ is a free parameter.

The coverage of $J$ is:
\begin{align}
    Q(J, \mu, \sigma) 
    &= \int \dd x_1\ldots \dd x_n\, p(x_1,\ldots, x_n)\,
    I\left(\bar{x} - \frac{z s}{\sqrt{n}} < \mu 
        < \bar{x} + \frac{z s}{\sqrt{n}}\right)\\
    &= \int \dd x_1\ldots \dd x_n\, p(x_1,\ldots, x_n)\,
    I\left(-z < t < z\right)\\
    & = \int \dd t\ \mathrm{PDF}_{\mathrm{Student}}(t|n - 1) I(-z < t < z) \\
    & = \mathrm{CDF}_{\mathrm{Student}}(z|n - 1) - 
        \mathrm{CDF}_{\mathrm{Student}}(-z|n - 1)\,.
\end{align}

Thus the coverage of $J$ does \emph{not} depend on the parameters $(\mu,
\sigma)$ of the model, and can be set to be equal to any prescribed value
$\alpha \in (0, 1)$ by choosing appropriately the parameter $z$.

Therefore, the confidence intervals produced by $J$ are exact, and saturate the
minimum coverage bound for all values of $(\mu, \sigma)$. However, it is worth
noting that the intervals only constrain the mean parameter $\mu$, and do not
give any guarantees on the variance parameter $\sigma^2$.

\section{Poisson distribution}

Let $x \in \mathbb{N}$ be a single observation from a Poisson distribution with
mean $\lambda$:
\begin{equation}
    \mathrm{PMF}_{\mathrm{Poisson}}(x|\lambda) = 
    \frac{\lambda^x e^{-\lambda}}{x!}\,.
\end{equation}

Two confidence interval functions that constrain the parameter $\lambda$ from
below and from above are:
\begin{align}
    &J^{-}_{\alpha}(x) = \left\{\lambda\ |\
        \lambda > \lambda_{\mathrm{min}}(x, \alpha) \right\}\\
    &J^{+}_{\alpha}(x) = \left\{\lambda\ |\
        \lambda < \lambda_{\mathrm{max}}(x, \alpha) \right\},
\end{align}
where:
\begin{align}
    &\lambda_{\mathrm{min}}(x, \alpha) = \frac{1}{2}\mathrm{Quantile}_{\chi^2}
    \left(1 - \alpha, 2  x\right), \\
    &\lambda_{\mathrm{max}}(x, \alpha) = \frac{1}{2}\mathrm{Quantile}_{\chi^2}
    \left(\alpha, 2 (x + 1\right)).
\end{align}

The first few values of the bounds $\lambda_{\mathrm{min}/\mathrm{max}}$ are
shown in Table~\ref{tab:poisson_bounds}.

\begin{table}
\begin{center}
\begin{tabular}{|rrr|rrr|rrr|}
\hline
    \multicolumn{3}{|c|}{$\alpha = 0.95$} &
    \multicolumn{3}{|c|}{$\alpha = 0.975$} &
    \multicolumn{3}{|c|}{$\alpha = 0.995$}\\
\hline
    x &     $\lambda_{\mathrm{min}}$ &   $\lambda_{\mathrm{max}}$ &
    x &     $\lambda_{\mathrm{min}}$ &   $\lambda_{\mathrm{max}}$ &
    x &     $\lambda_{\mathrm{min}}$ &   $\lambda_{\mathrm{max}}$ \\
\hline
\hline
 0 & 0.000 &  2.996 & 0 & 0.000 &  3.689 & 0 & 0.000 &  5.298 \\
 1 & 0.051 &  4.744 & 1 & 0.025 &  5.572 & 1 & 0.005 &  7.430 \\
 2 & 0.355 &  6.296 & 2 & 0.242 &  7.225 & 2 & 0.103 &  9.274 \\
 3 & 0.818 &  7.754 & 3 & 0.619 &  8.767 & 3 & 0.338 & 10.977 \\
 4 & 1.366 &  9.154 & 4 & 1.090 & 10.242 & 4 & 0.672 & 12.594 \\
 5 & 1.970 & 10.513 & 5 & 1.623 & 11.668 & 5 & 1.078 & 14.150 \\
 6 & 2.613 & 11.842 & 6 & 2.202 & 13.059 & 6 & 1.537 & 15.660 \\
 7 & 3.285 & 13.148 & 7 & 2.814 & 14.423 & 7 & 2.037 & 17.134 \\
 8 & 3.981 & 14.435 & 8 & 3.454 & 15.763 & 8 & 2.571 & 18.578 \\
 9 & 4.695 & 15.705 & 9 & 4.115 & 17.085 & 9 & 3.132 & 19.998 \\
\hline
\end{tabular}
\end{center} 
\caption {\label{tab:poisson_bounds} Bounds for exact Poisson confidence
intervals.}
\end{table}

The one-sided confidence intervals $J^{\pm}_\alpha$ have coverage not less than
$\alpha$ for all values of $\lambda$, i.e. they are exact, although they do not
saturate the minimum coverage bound for all values of $\lambda$. In fact, no
choice of $J$ can do so. They can be combined to give an exact two sided
confidence interval with coverage $2\alpha - 1$:
\begin{equation}
    J_{2 \alpha - 1}(x) = \left\{
    \lambda\ |\ \lambda_{\mathrm{min}}(x, \alpha) < \lambda 
    <  \lambda_{\mathrm{max}}(x, \alpha) \right\}.
\end{equation}

The coverage of $J^{\pm}_{\alpha}$ and $J_{2\alpha - 1}$ as a function of
$\lambda$ is shown in Figure~\ref{fig:poisson_coverage}. Note how the coverage
changes abruptly when the Poisson parameter $\lambda$ crosses one of the bounds
$\lambda_{\mathrm{min}/\mathrm{max}}$. However, despite these fluctuations, the
coverage of $J_{\alpha \pm}$ always remains above $\alpha$, and the coverage of
$J_{2\alpha - 1}$ always remains above $2\alpha - 1$.

\begin{figure}
\begin{center}
    \includegraphics[scale=0.75]{poisson_coverage.png}
\end{center}
\caption{\label{fig:poisson_coverage}Coverage of the exact Poisson confidence
intervals.}
\end{figure}
\end{document}
