\documentclass[10pt,letterpaper]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{graphicx}
\usetikzlibrary{positioning}
\renewcommand{\vec}[1]{\boldsymbol{\1}}
\newcommand{\cov}[2]{\mathrm{cov}\!\left[#1,\,#2\right]}
\newcommand{\transp}{\mathrm{T}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\ee}{\mathrm{e}}
\newcommand{\normal}[2]{N\!\left(#1,\,#2\right)}
\newcommand{\expectation}[2]{\mathrm{E}_{#1}\!\left[#2\right]}
\title{Data from noise}
\author{Andrea Allais}
\begin{document}
\maketitle

\section{Diffusion}

The goal of generative modeling is to map a random variable with a known and
simple distribution (\textit{e.g.} the normal distribution), to a random
variable distributed like the data:
\begin{equation}
    x \sim \normal{0}{I}\,; \quad y = f(x) \sim \mathrm{Data}\,.
\end{equation}

The function $f$ is to be learned from a sample from the data distribution.  At
this level it is not at all clear how a learning objective may be specifed for
$f$. One approach is learning to reverse a diffusion process in which small
amounts of noise are progressively added to the data variable $y$ until it
becomes essentially pure noise.

The diffusion process is described at a high level by the graphical model:
\begin{center}
\begin{tikzpicture}[every node/.style={draw,circle,minimum size=1cm}]
    \node (x0) {$x_0$};
    \node (x1) [right=of x0] {$x_{\Delta t}$};
    \node (x2) [right=of x1] {$x_{2\Delta t}$};
    \node (x3) [right=of x2] {$x_{3\Delta t}$};
    \node (x4) [right=of x3] {$\ldots$};
    \node (z1) [above=of x1] {$z_{\Delta t}$};
    \node (z2) [above=of x2] {$z_{2\Delta t}$};
    \node (z3) [above=of x3] {$z_{3\Delta t}$};
    \path (x0) edge[->] (x1)
          (x1) edge[->] (x2)
          (x2) edge[->] (x3)
          (x3) edge[->] (x4)
          (z1) edge[->] (x1)
          (z2) edge[->] (x2)
          (z3) edge[->] (x3);
\end{tikzpicture}
\end{center}
and in detail by the recurrence relation:
\begin{equation}
    x_{t + \Delta t} = x_{t} - \frac{1}{2} g^2(t) \Delta t\, x_t + g(t) \sqrt{\Delta t}\, z_{t}\,,
\end{equation}
where:
\begin{enumerate}
\item $\{x_t\}$ is a set of random variables, one for each value of $t$. The
    first one, $x_0$, is distributed like the data, and the following are
    progressively noisier.
\item $z_t \sim \normal{0}{I}$ is the noise variable that is added at each step.
    $z_t$ is independent from $z_s$ for all $t \neq s$, and is also independent
    from $x_s$ for $s \leq t$.
\item $g$ is a function parameter, sometimes called noise schedule, that
    controls the amount of noise that is injected at every step of the
    diffusion process
\item $\Delta t$ is a discretization step. In the limit $\Delta t \to 0$ the
    recurrence relation becomes a stochastic differential equation. Results
    below are given in this limit.
\end{enumerate}

Many properties of the diffusion process depend on the noise schedule $g$ only
through the positive, monotonically decreasing function:
\begin{equation}
    \Lambda(t) = \ee^{-\frac{1}{2} \int_0^{t} g^2(s)\, \dd s}\,.
\end{equation}
In particular, $\Lambda(t) \to 0$ for $t \to \infty$ (unless $g$ goes to zero
faster than $1/t$).


Here are some of those properties that are relevant for inverting the process
\begin{enumerate}
    \item $\expectation{}{x_t} = \Lambda(t) \expectation{}{x_0}$, and in particular
        $\expectation{}{x_t}\to 0$ for $t \to \infty$.
    \item $\cov{x_t}{x_t^{\transp}} = I + \Lambda^2(t) (\cov{x_0}{x_0^\transp} - I)$, 
        and in particular $\cov{x_t}{x_t^{\transp}} = I$ for $t \to \infty$.
    \item The joint distribution of any set of variables $(x_{t_1}, x_{t_2},
        \ldots)$ \emph{conditioned on  $x_0$} is normal. This is because the
        recurrence relation is linear in the random variables.
    \item In particular, $x_t|x_0 \sim \normal{\Lambda(t) x_0}{(1 -
        \Lambda^2(t)) I}$. This makes it efficient to sample $x_t$ both
        conditionally on $x_0$ and unconditionally, without using ancestral
        sampling through the recurrence relation.
    \item $x_t \sim \normal{0}{I}$ for $t \to \infty$. Thus $x_t$ for large $t$
        is the noise variable from which we want to learn to generate $x_0$.
\end{enumerate}

Below is an illustrative one-dimensional example of the diffusion process. On
the left is the ``data'' distribution, which we take to be a mixture of two
narrow normal distributions. On the right is the final distribution
$\normal{0}{I}$. In the middle is a contour plot of the PDF of $x_t$, and
overlayed as white lines are 16 samples from the process. For this example
$g(t) = 1$.

\begin{center}
    \includegraphics[scale=0.5]{diffusion.png}
\end{center}

\section{Reverse diffusion}

Remarkably, the joint distribution of $x_t$ specified by the diffusion process
can also be obtained in reverse, \textit{i.e.} according to the graphical
model:
\begin{center}
\begin{tikzpicture}[every node/.style={draw,circle,minimum size=1cm}]
    \node (x0) {$x_0$};
    \node (x1) [right=of x0] {$x_{\Delta t}$};
    \node (x2) [right=of x1] {$x_{2\Delta t}$};
    \node (x3) [right=of x2] {$x_{3\Delta t}$};
    \node (x4) [right=of x3] {$\ldots$};
    \node (z1) [above=of x1] {$\bar{z}_{\Delta t}$};
    \node (z2) [above=of x2] {$\bar{z}_{2\Delta t}$};
    \node (z3) [above=of x3] {$\bar{z}_{3\Delta t}$};
    \node (z4) [above=of x4] {$\bar{z}_{4\Delta t}$};
    \path (x0) edge[<-] (x1)
          (x1) edge[<-] (x2)
          (x2) edge[<-] (x3)
          (x3) edge[<-] (x4)
          (z1) edge[->] (x0)
          (z2) edge[->] (x1)
          (z3) edge[->] (x2) 
          (z4) edge[->] (x3);
\end{tikzpicture}
\end{center}
through the recurrence relation:
\begin{equation}
    x_{t - \Delta t} = x_t + g^2(t) \Delta t \left[\frac{x_t}{2} + 
    \nabla \log p(x_t, t) \right] + g(t) \sqrt{\Delta t}\, \bar{z}_t
\end{equation}
where:
\begin{enumerate}
    \item $x_t \sim \normal{0}{I}$ for $t \to \infty$
    \item $\bar{z}_t \sim \normal{0}{I}$. $\bar{z}_t$ is independent from
        $\bar{z}_s$ for all $t \neq s$, and also from $x_s$ for $s \geq t$.
    \item $p(x,t)$ is the marginal PDF of $x_t$, \textit{i.e.}
        \mbox{$\mathrm{Pr}[x_t\in D] = \int_D p(x, t) \dd x$} for any domain
        $D$. 
\end{enumerate}

The joint distribution of $x_t$ generated by this recurrence relation is the
same as the one generated by the forward diffusion process. In particular,
$x_0 \sim \mathrm{Data}$, and the figure in the previous section is also an
entirely valid illustration of this reverse process.

The recurrence relation is not linear. This is necessary to generate a
non-normal distribution for $x_0$ from normally distributed $x_\infty$ and
$z_t$. Therefore, unlike the forward process, there is no way to sample the
process more efficiently than by using the recurrence relation.

\begin{center}
    \includegraphics[scale=0.5]{score_function.png}
\end{center}

The relation depends on the so-called ``score function`` $\nabla \log p(x, t)$.
This is a vector field that points everywhere towards regions of high
probability, and drives the reverse diffusion process there.  The practical use
of reverse diffusion to sample from the data distribution depends on the
ability to evaluate the score function efficiently.

\section{Score matching $\equiv$ denoising autoencoding}

An efficient objective can be specified to learn a parametric approximation
$s_\theta(x, t)$ to the score function $\nabla \log p(x, t)$. The objective is
the naive one:
\begin{equation}
    L(\theta, t) = \expectation{x_t}{\left|s_{\theta}(x_t, t) - \nabla \log p(x_t, t)\right|^2}\,,
\end{equation}
but made tractable thanks to the non-trivial equivalence:
\begin{equation}
    L(\theta, t) = \expectation{x_0, x_t}{\left|s_{\theta}(x_t, t) - \nabla \log p(x_t | x_0, t)\right|^2}\,,
\end{equation}
where $p(x_t|x_0, t)$ is the PDF of $x_t$ \emph{conditional} on $x_0$.

The second form is tractable because, as discussed before:
\begin{equation}
    x_t|x_0 \sim \normal{\Lambda(t) x_0}{(1 - \Lambda^2(t)) I}
\end{equation}
and hence:
\begin{equation}
    L(\theta, t) = \expectation{x_0, x_t}{\left|s_{\theta}(x_t, t) - 
    \frac{\Lambda(t)x_0 - x_t}{1 - \Lambda^2(t)}\right|^2}\,.
\end{equation}

The learning objective can be interpreted as the objective for a denoising
autoencoder:
\begin{enumerate}
    \item Sample $x_0$ from the data distribution.
    \item Mix it with Gaussian noise $z$ to obtain $x_t = \Lambda\, x_0 +
        \sqrt{1 - \Lambda^2}\, z$.
    \item Regress the added noise $z / \sqrt{1 - \Lambda^2}$ from the noisy
        variable $x_t$ to obtain $s_\theta(x, t)$.
\end{enumerate}

\section{Probability flow}

It is also possible to construct an ordinary differential equation that
reproduces the marginal distribution of $x_t$ according to the diffusion
process, but not the joint distribution of multiple variables $(x_{t_0},
x_{t_1}, ...)$:
\begin{equation}
    \frac{\dd x_t}{\dd t} = - \frac{1}{2} g^2(t) \left[x_t + \nabla \log p(x_t, t) \right]\,.
\end{equation}

This ODE is known as the ``probability flow'' ODE, and it yields a
deterministic, invertible map between $x_\infty \sim \normal{0}{I}$ and $x_0
\sim \mathrm{Data}$.  The trajectories generated by this ODE are illustrated in
the figure below:
\begin{center}
    \includegraphics[scale=0.5]{probability_flow.png}
\end{center}

\section{Flow matching}

A diffusion process is not the only way to smoothly interpolate between the
data distribution and a normal distribution. Another, arguably simpler option
is:
\begin{equation}
    x_t = \Lambda(t)\, x_0 + \sigma(t)\, z\,,
\end{equation}
where:
\begin{enumerate}
    \item $x_0 \sim \mathrm{Data}$
    \item $z \sim \normal{0}{I}$
    \item $\Lambda$ and $\sigma$ is any pair of functions such that $\Lambda(0)
        = 1$, $\sigma(0) = 0$ (for self-consistency $x_0 = x_0$), and
        $\Lambda(1) = 0$, $\sigma(1) = 1$, so that $x_1 \sim \normal{0}{I}$.
        For example $\Lambda(t) = 1 - t$ and $\sigma(t) = t$.
\end{enumerate}

The marginal distribution of $x_t$ according to  the diffusion process can also
be cast in this form, by a special choice of $\Lambda$ and $\sigma$:
\begin{equation}
    \Lambda(t) = \ee^{-\frac{1}{2} \int_0^{t} g^2(s)\, \dd s}\quad \mathrm{and}\quad \sigma(t) = 1 - \Lambda^2(t)\,,
\end{equation}
although in this case normality is attained for $t \to \infty$ rather than $t = 1$.

A probability flow ODE can be constructed for this process:
\begin{equation}
    \frac{\dd x_t}{\dd t} = -u(x_t, t),
\end{equation}
where the vector field $u$ is given by:
\begin{equation}
    u(x_t, t) = \frac{1}{p(x_t, t)} \expectation{x_t}{u(x_t | x_0, t)\, p(x_t|x_0, t)}\,,
\end{equation}
and:
\begin{equation}
    u(x_t|x_0, t) = - \dot{\Lambda}(t) x_0 + \frac{\dot{\sigma}(t)}{\sigma(t)} (x_t - \Lambda(t) x_0)\,.
\end{equation}

The ODE is tractable only if there is an efficient parametric approximation
$v_\theta(x, t)$ to the vector field $u(x, t)$. Such an approximation can be
learned in a similar way to score matching; an approach called ``flow
matching'':
\begin{align}
    L(\theta, t) &= \expectation{x_t}{\left|v_\theta(x_t, t) - u(x_t, t)\right|^2} \\
    & = \expectation{x_0, x_t}{\left|v_\theta(x_t, t) - u(x_t|x_0, t)\right|^2}
\end{align}

The solution to the flow matching ODE is illustrated in the figure below:
\begin{center}
    \includegraphics[scale=0.5]{flow_matching.png}
\end{center}

Flow matching has some advantages over diffusion-based probability flow:
\begin{enumerate}
    \item A normally distributed variable is obtained at $t = 1$ exactly. In
        contrast, the diffusion process only approaches normality for $t \to
        \infty$, although this could be obviated by using a singular noise
        schedule $g^2(t) = 1/(1-t)$.
    \item Empirically, the flow-matching learning objective appears to be more
        stable and robust.
    \item Better FID metrics on ImageNet.
\end{enumerate}

\section{References}

\begin{enumerate}
    \item \textbf{Anderson (1982), Reverse-time diffusion equation models.}
        First derivation of the stochastic differential equation that describes
        the reverse diffusion process.
    \item \textbf{Vincent (2011), A Connection Between Score Matching and
        Denoising Autoencoders.}
    \item \textbf{Sohl-Dickstein \textit{et al.} (2015), Deep Unsupervised
        Learning using Nonequilibrium Thermodynamics}. First time learning to
        reverse the diffusion process. It does not use the language of SDEs and
        is not aware of the connection to score matching, but the substance is
        the same as what is described here.
    \item \textbf{Song, Ermon (2020), Generative Modeling by Estimating
        Gradients of the Data Distribution} First time the lanuage of SDEs is
        applied to the problem, and the connection to score matching is
        exploited. Interesting discussion on how the noise in score matching
        improves learning stability and the coverage of multiple distribution
        modes.
   \item \textbf{Ho textit{et al.} (2020), Denoising Diffusion Probabilistic
       Models}. Contemporanous to the previous paper, and makes roughly the
        same points.
   \item \textbf{Song \textit{et al.} (2021), Score-Based Generative Modeling
       through Stochastic Differential Equations}. First time the probability
        flow ODE is introduced.
   \item \textbf{Lipman \textit{et al.} (2023), Flow Matching for Generative
       Modeling}. First time the flow matching ODE and learning objective are
        introduced.
\end{enumerate}

\end{document}
